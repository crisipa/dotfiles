# /proc/sys/net/core/*

# 固化 BPF JIT 编译器
# 0 - 禁用 JIT 强化（默认值）
# 1 - 仅为非特权用户启用 JIT 强化
# 2 - 为所有用户启用 JIT 强化
net.core.bpf_jit_harden  =  2

# 设置用于网络设备的默认队列规则
# 在内核编译配置中搜索 CONFIG_NET_SCH_ 来查看内核的支持
# 可以 modprobe sch_* 相关模块来添加拥塞控制算法
# 请注意 tcp 拥塞控制算法
# net.ipv4.tcp_congestion_control
# 不正确的搭配会导致网络使用体验劣化
# 如高丢包率等问题
# choke codel pfifo bfifo fq fq_codel fq_pie gred hhf ingress mqprio multiq netem pfifo_fast pie red sfb sfq tbf
# pfifo_fast fq codel fq_codel fq_pie sfq
# 可在内核编译时选择为默认默认队列规则的队列规则
# pfifo_fast
# 默认的队列规则
# 基于传统的FIFO qdisc
# 提供了一些基于优先级的处理
# 有三个优先级
# 根据 ToS 等来划分优先级
# 最高优先级的流量(交互式流量)总是会被优先处理
# 在前一个优先级的队列清空前
# 后一个优先级的队列不会被处理
# fq    Fair Queue    公平队列
# 实现相对简单
# 旨在通过公平地分配带宽来提高网络性能
# 它确保每个流量都能公平地获得带宽 避免某些流量占用过多资源
# codel Controlled Delay
# 旨在通过设置缓冲区中网络数据包的延迟限制来克服网络硬件如路由器中的缓冲膨胀
# 改善了随机早期检测（RED）算法的整体性能
# 设计上 CoDel 比 RED 更容易管理和配置
# fq_codel  公平队列控制延迟
# fq 和 codel 的结合
# 期望更好地流之间分配可用带宽
# fq_pie    Flow Queuing with Proportional Integral controller Enhanced
# 将流队列与 PIE AQM 方案相结合的排队规则
# 为使用 qdisc 的所有流提供公平的带宽份额
# sfq   随机公平队列
# 该队列规则试图在任意数量的流之间公平地分配传输数据的机会
# 它使用Hash函数分离流量
# 基本没多大用
# pfifo bfifo noqueue
# 除去上面的，基本上会有的队列规则
# FIFO 先进先出
# pfifo bfifo
# pfifo Packet limited First In, First Out queue    单位是数据包
# bfifo Byte limited First In, First Out queue      单位是字节
# 最简单的队列规则 先入先出
# 不会对报文进行整流和重排
# 仅在接收到报文并在报文入队列后尽快将其发送出去
# noqueue
# 不排队
# 没有调度程序
# 丢弃所有排队的包
# 没有速率限制
# 不对数据包进行分类
# pie cake
# 其他一些常见队列规则
# cake  Common Applications Kept Enhanced
# 同时使用 AQM 和 FQ 的整形队列规则
# 会尽量抢占宽带
# 也会用队列系统帮路由器上的应用分配宽带
# 适合用在路由器上
# pie   Proportional Integral controller-Enhanced
# 主要设计目标
# 低延迟控制
# 链路利用率高
# 简单实现
# 保证稳定性和快速响应
# tbf htb prio multiq netem
# mq mqprio
# 别的队列规则
# TBF   令牌桶过滤器
# 它会对接口上传输的流量进行整形(支持整流)
# 可以限制特定接口上出队列的报文的速度
# HTB   Hierarchical Token Bucket   分层令牌桶
# 允许用户定义所使用的令牌和桶的特征，并允许用户以任意方式嵌套这些桶
# 可以以非常精细的方式控制流量
# PRIO  Priority Scheduler 优先级调度器
# 实际上不会整形，只会根据设置的过滤器对流量进行分类
# 可以理解为 pfifo_fast 的升级版
# 和 sfq 一样没用
# netem network emulator
# 可以模拟 网络延迟 丢包 重复 数据包损坏 等网络特征
# multiq
# Hardware Multiqueue-aware Multi Band Queuing
# 针对具有多个 Tx 队列的设备优化的队列规则
# 类似于 pfifo_fast ？
# net.core.default_qdisc = fq_codel

# 接收队列的大小
# 在使用高速网卡时增加系统的这个设定值可能有助于防止数据包丢失
# net.core.netdev_max_backlog = 1024

# 最大连接数
# net.core.somaxconn = 4096

# /proc/sys/net/ipv4/*

# IP 协议的 early demux 选项
# 纯转发设备应关闭它
# 默认值：1
# net.ipv4.ip_early_demux = 0

# 开启转发支持
net.ipv4.ip_forward = 1

# 设置 tcp 拥塞控制算法
# 下面两个变量表示你目前可用的拥塞控制算法
# 在内核编译配置中搜索 CONFIG_TCP_CONG_ 来查看内核的支持
# 也可以 modprobe tcp_* 相关模块来添加拥塞控制算法
# net.ipv4.tcp_allowed_congestion_control
# net.ipv4.tcp_available_congestion_control
# 同时请注意默认队列规则
# net.core.default_qdisc
# 不正确的搭配会导致网络使用体验劣化
# 如高丢包率等问题
# reno cubic westwood bbr
# 较为常见
# reno
# 基于丢包反馈
# 带宽利用率不高
# cubic
# 基于丢包反馈
# 一种较为温和的拥塞算法
# 为 linux 默认拥塞算法
# westwood
# 基于延时反馈
# 根据 ACK 包接收速率来估算可用带宽
# 不能很好的区分传输过程中的拥塞丢包和无线丢包
# 适用于无线环境
# BBR
# 基于丢包反馈
# 与 fq pie fq_pie cake 搭配？
# BBRv1
# 并不适合任何网络环境
# 设计上配合了 http/2 的大流传输特点
# 拥有启动快抖动小等优点
# 由于不以丢包为减小窗口的判断依据
# 能很好的对付大带宽传输中的少量丢包
# 但是在总可用带宽较小时
# 降速不够迅速准确
# 自身带宽利用率低还会影响同一网络下的其他流
# 因此 BBRv1 适合丢包率不太高
# 的长胖管道（Long-Fat Network，指延迟较高且带宽较大）
# 据 aws 文章
# BBRv2 在 BBRv1 基础上有了很大的改进
# 而且在需要更高带宽的情况下
# 它更接近于成为 CUBIC 的完全替代品
# net.ipv4.tcp_congestion_control = cubic

# 为 TCP 启用早期多路复用
# 默认值：1
# net.ipv4.tcp_early_demux = 1

# 是否开启 TLP(Tail loss probe) 机制
# TLP 机制需要 RACK 机制才能正常工作
# 0     关闭
# 3/4   启用
# 默认值：3
# net.ipv4.tcp_early_retrans = 3

# 控制 TCP 使用的 Explicit Congestion Notification (ECN) 机制
# ECN 只有在 TCP 连接双方协商支持时才启用
# 这个功能允许路由在丢包之前就通知有拥塞的存在，以此来减少因为真正拥塞导致的丢包
# 默认值：2
# 0: 关闭 ECN，不主动开启也不被动响应
# 1: 开启 ECN，主动开启，被动响应
# 2: 开启 ECN，不主动开启，被动响应
# net.ipv4.tcp_ecn = 1

# 如果内核检测到 ECN 连接行为异常，则回退到非ECN
# 如果tcp_ecn或路由（或拥塞）ECN 设置被禁用，则不使用该值
# 默认 1
net.ipv4.tcp_ecn_fallback = 1

# 启用 TCP Fast Open
# 1     客户端，允许客户端在 SYN 中携带数据
# 2     服务端，开启服务端支持，允许在三次握手结束前接收数据并传递给应用程序
# 4     客户端，不管 FTO cookie 是否存在，都在 SYN 中发送数据，且不带 cookie 选项
# 200   服务端，没有 cookie 选项时依旧接收 SYN 报文中的数据
# 400   服务端，为所有监听端口开启 FTO，不用为端口单独设置 TCP_FASTOPEN 选项
net.ipv4.tcp_fastopen = 3

# TFO 防火墙黑洞问题发生时停用 TFO 的时长 以秒为单位
# 0 禁用
net.ipv4.tcp_fastopen_blackhole_timeout_sec = 60

# 启用RFC5682中定义的前向 RTO 恢复
# F-RTO 是一种用于 TCP 重传的增强恢复算法
# 这在网络中有 RTT 波动（例如，无线）时有效
# F-RTO不需要对等方的任何支持
# 默认情况下，它使用非零值启用
# 1 开启基本版本的F-RTO算法
# 2 如果流使用SACK的话，开启SACK-增强的F-TRO算法 （默认）
net.ipv4.tcp_frto = 2

# 启用 keepalive 时 TCP 发送 keepalive 消息的频率。 默认值：2 小时
net.ipv4.tcp_keepalive_time = 60
# 探测器发出的频率。默认值：75秒
net.ipv4.tcp_keepalive_intvl = 10
# TCP 发出多少个 keepalive 探测器，直到它决定断开连接。默认值：9。
net.ipv4.tcp_keepalive_probes = 6

# 挂起连接等待确认的最大队列长度
# net.ipv4.tcp_max_syn_backlog = 4096

# TCP 接收缓冲区自动调整
# 默认启用
net.ipv4.tcp_moderate_rcvbuf = 1

# 启用 MTU 探测
# 0  =  禁用
# 1  =  默认禁用, 在检测到 ICMP black hole 时启用
# 2  =  始终启用
net.ipv4.tcp_mtu_probing = 2

# Protective Load Balancing
# 需要 tcp 拥塞控制算法 的支持
# 目前仅支持 ipv6
# 默认未启用
# net.ipv4.tcp_plb_enabled = 1

# 开启一些还在实验的丢包恢复的功能
# 0x1，（默认）为丢包重传和丢尾包的情况开启 RACK 丢包检测，对 SACK 连接来说，它已经包含了 RFC6675 的恢复并且禁用相关功能
# 0x2，使用静态的 RACK 重排序窗口，置为 (min_rtt/4)
# 0x4，不使用 RACK 的启发式 DUPACK 阈值
# net.ipv4.tcp_recovery = 1

# SACK
# Enable select acknowledgments (SACKS).
net.ipv4.tcp_sack = 1

# 这将更改 TCP 接收窗口的计算方式
# 0 已禁用 窗口永远不会缩小
# 1 已启用 必要时窗口会缩小
# 启用可能会导致网速下降
# net.ipv4.tcp_shrink_window = 1

# 设置 TCP 是否应仅对新连接或空闲时间过长的现有连接开启'以默认窗口大小启动连接'
# 默认值：1
# 此设置会破坏持久的单连接性能，可以将其关闭
net.ipv4.tcp_slow_start_after_idle = 0

# TCP SYN cookie 保护
# 不合 RFC 规范
net.ipv4.tcp_syncookies = 0

# TCP 时间戳
# 1：开启功能并为每个连接使用随机的偏移量而不是只使用当前时间
# 2：与 1 类似，但没有随机偏移量
net.ipv4.tcp_timestamps = 1

# 是否为 thin stream 开启线性超时重传
# 对一些依赖低延时的小流量数据流，可以减小重传的延时
net.ipv4.tcp_thin_linear_timeouts = 1

# 当新时间戳严格大于上一个连接记录的最新时间戳时
# 是否应该为新的 TCP 传出连接重用 TIME-WAIT 状态下的现有连接
# 需要启用 net.ipv4.tcp_timestamps
# 1 会导致一些问题
# 0 - 禁用
# 1 - 全局启用
# 2 - 仅为环回流量启用
# net.ipv4.tcp_tw_reuse = 1

# 启用RFC1323中定义的窗口缩放
net.ipv4.tcp_window_scaling = 1

# 为 UDP 启用早期解复用
# 默认值：1
# net.ipv4.udp_early_demux = 1

# 禁用 ICMP 重定向
net.ipv4.conf.default.accept_redirects = 0
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.default.secure_redirects = 0
net.ipv4.conf.all.secure_redirects = 0
# 路由器请注释下面两行
# net.ipv4.conf.default.send_redirects = 0
# net.ipv4.conf.all.send_redirects = 0

# 在接口之间转发数据包
net.ipv4.conf.default.forwarding = 1
net.ipv4.conf.all.forwarding = 1

# 接受具有本地源地址的数据包
# 默认 0
# net.ipv4.conf.default.accept_local = 1
# net.ipv4.conf.all.accept_local = 1

# 在路由时不要将环回地址视为 martian 源或目标
# 这允许将 127/8 用于本地路由目的
# 默认 0
# net.ipv4.conf.default.route_localnet = 1
# net.ipv4.conf.all.route_localnet = 1

# 反向路径过滤
# 值 1 严格模式 2 宽松模式
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1

# /proc/sys/net/ipv6/*

# 禁用 ICMP 重定向
net.ipv6.conf.default.accept_redirects = 0
net.ipv6.conf.all.accept_redirects = 0

# 在接口之间转发数据包
net.ipv6.conf.default.forwarding = 1
net.ipv6.conf.all.forwarding = 1

# 隐私扩展的首选项 （RFC3041）
# 0 禁用隐私扩展
# 1 启用隐私扩展，但首选公共 临时地址上的地址。
# 2 启用隐私扩展并首选临时 公共地址上的地址。
# Default: 0（适用于大多数设备） -1（用于点对点设备和环回设备）
net.ipv6.conf.default.use_tempaddr = 2
net.ipv6.conf.all.use_tempaddr = 2
net.ipv6.conf.lo.use_tempaddr = -1

# 临时地址的有效生命周期 以秒为单位
# 如果少于所需的最短生命周期则不会创建临时地址
# 决定临时地址能存在多久
# 最短生命周期通常为 5-7 秒
# 内核值: 172800 (2 days)
# net.ipv6.conf.default.temp_valid_lft = 14400
# net.ipv6.conf.all.temp_valid_lft = 14400

# 临时地址的首选生命周期 以秒为单位
# 超时后不会再用它发起连接
# 内核值: 86400 (1 day)
# net.ipv6.conf.default.temp_prefered_lft = 3600
# net.ipv6.conf.all.temp_prefered_lft = 3600

# 每个接口的最大自动配置地址数
# 设置为 0 将禁用限制
# 不建议将此值设置得太大或设置为零
# Default: 16
# net.ipv6.conf.default.max_addresses = 4
# net.ipv6.conf.all.max_addresses = 4

# 定义如何生成链接本地地址和 autoconf 地址。
# 0 基于 EUI64 生成地址（默认）
# 1 不生成链路本地地址，使用 EUI64 作为地址 从 autoconf 生成
# 3 生成稳定的隐私地址，如果未设置，则使用随机密钥
net.ipv6.conf.default.addr_gen_mode = 3
net.ipv6.conf.all.addr_gen_mode = 3
net.ipv6.conf.lo.addr_gen_mode = 0

# /proc/sys/net/mptcp/*

# 启用 Multi-path tcp
net.mptcp.enabled = 1

# 启用MPTCP传输层中数据序列号校验和（DSS-checksum）
# DSS-checksum主要和传输的可靠性相关
# 只要通信对端中有一端开启，就会执行。
net.mptcp.checksum_enabled = 1

# /proc/sys/net/netfilter/*

# 验证传入数据包的校验和
# 校验和错误的数据包处于无效状态
# 0 - disabled
# not 0 - enabled (default)
net.netfilter.nf_conntrack_checksum = 1

# 启用对连接时间戳的检查
# 0 - disabled (default)
# not 0 - enabled
net.netfilter.nf_conntrack_timestamp = 1

# tcp 连接建立超时
# tcp 连接建立后不发送数据的超时时间
# default 432000 (5 days)
net.netfilter.nf_conntrack_tcp_timeout_established = 150
